
# üìù Publications 
## üéº Automatic Target Recognition
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TAES 2023</div><img src='images/TAES_2023.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Contrastive Learning and Cycle Consistency-Based Transductive Transfer Learning for Target Annotation](https://doi.org/10.1109/TAES.2023.3337768) \\
Shoaib Meraj Sami, **Md Mahedi Hasan**, Nasser Nasrabadi, Raghuveer Rao [code]()
- We propose a hybrid contrastive learning base unpaired domain translation (H-CUT) network that achieves a significantly lower FID score. It incorporates both attention and entropy to emphasize the domain-specific region, a noisy feature mixup module to generate high variational synthetic negative patches, and a modulated noise contrastive estimation (MoNCE) loss to reweight all negative patches using optimal transport for better performance. 
- Our proposed contrastive learning and cycle-consistency-based TTL (C3TTL) framework consists of two H-CUT networks and two classifiers.
</div>
</div>

## üßë‚Äçüé® Face Recognition
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">WACV 2024</div><img src='images/wacv_2024.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Text-Guided Face Recognition using Multi-Granularity Cross-Modal Contrastive Learning](https://arxiv.org/abs/2312.09367) \\
**Md Mahedi Hasan**, Shoaib Meraj Sami, and Nasser Nasrabadi. [code]() [video](https://www.youtube.com/watch?v=Hb8SlpFCuGI)

- We introduce text-guided face recognition (TGFR) to analyze the impact of integrating facial attributes in the form of natural language descriptions while we hypothesize that adding semantic information into the loop can significantly improve the image understanding capability of an FR algorithm compared to other soft biometrics. 
- We also design a face-caption alignment module (FCAM), which incorporates cross-modal contrastive losses across multiple granularities to maximize the mutual information between local and global features of the face-caption pair. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCB 2024</div><img src='images/IJCB_2023.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Improving Face Recognition from Caption Supervision with Multi-Granular Contextual Feature Aggregation](https://arxiv.org/abs/2308.06866) \\
**Md Mahedi Hasan**, and Nasser Nasrabadi. [code]()
-  We introduce caption-guided face recognition (CGFR) as a new framework to improve the performance of commercial-off-the-shelf (COTS) face recognition (FR) systems. 
- We propose a contextual feature aggregation module (CFAM) that addresses this issue by effectively exploiting the fine-grained word-region interaction and global image-caption association. Specifically, CFAM adopts a self-attention and a cross-attention scheme for improving the intra-modality and inter-modality relationship between the image and textual features, respectively. 
- We also design a textual feature refinement module (TFRM) that refines the textual features of the pre-trained BERT encoder by updating the contextual embeddings. 
</div>
</div>

## üìö Fingerprint Recognition 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IET Biometrics 2023</div><img src='images/IET_Biometrics_2023.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[On Improving Interoperability for Cross-Domain Multi-Finger Fingerprint Matching Using Coupled Adversarial Learning](https://doi.org/10.1049/bme2.12117) \\
**Md Mahedi Hasan**, Nasser Nasrabadi, and Jeremy Dawson 
- We project both the contactless and the contact-based fingerprint into a latent subspace to explore the hidden relationship between them using class-specific contrastive loss and ArcFace loss. 
- The ArcFace loss ensures intra-class compactness and inter-class separability, whereas the contrastive loss minimizes the distance between the subspaces for the same finger. 
- Experiments on four challenging datasets demonstrate that our proposed model outperforms two top-performing commercial-off-the-shelf SDKs, i.e., Verifinger v12.0 and Innovatrics.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">BIOSIG 2022</div><img src='images/BIOSIG.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Deep Coupled GAN-Based Score-Level Fusion for Multi-Finger Contact to Contactless Fingerprint Matching](https://doi.org/10.1109/BIOSIG55365.2022.9897056) <span style="color:red">[Oral Presentation]</span>\\
**Md Mahedi Hasan**, Nasser Nasrabadi, and Jeremy Dawson.  

-  To improve the interoperability between contact to contactless images in fingerprint matching, we propose a coupled deep learning framework that consists of two Conditional Generative Adversarial Networks.
-  Generative modeling is employed to find a projection that maximizes the pairwise correlation between these two domains in a common latent embedding subspace. 
</div>
</div>


## üéº Gait Recognition
- `IET Computer Vision 2021` [Learning view-invariant features using stacked autoencoder for skeleton-based gait recognition](https://doi.org/10.1049/cvi2.12050), **Md Mahedi Hasan**, and Hossen Asiful Mustafa. 
- `IJCSIS 2021` [Multi-level feature fusion for robust pose-based gait recognition using RNN](https://www.academia.edu/download/61987403/03_Paper_01012007_IJCSIS_Camera_Ready_pp20-3120200204-65998-gq509w.pdf), **Md Mahedi Hasan**, and Hossen Asiful Mustafa. <strong><span class='show_paper_citations' data='m3MlVBUAAAAJ:r0BpntZqJG4C'></span></strong>


## Others
- `ETCCE 2021` [A deep Spatio-temporal network for vision-based sexual harassment detection](https://doi.org/10.1109/ETCCE54784.2021.9689891), Md Shamimul Islam, **Md Mahedi Hasan**, Sohaib Abdullah, et al.
- ``Book Chapter 2020`` [Deep Learning based Early Detection and Grading of Diabetic Retinopathy Using Retinal Fundus Images](http://dx.doi.org/10.1201/9781003031352-6), Sheikh Muhammad Saiful Islam, **Md Mahedi Hasan**, Sohaib Abdullah <strong><span class='show_paper_citations' data='m3MlVBUAAAAJ:R3hNpaxXUhUC'></span></strong> [arXiv](https://arxiv.org/abs/1812.10595)
- `RAAICON 2019` [Robust Pose-Based Human Fall Detection using Recurrent Neural Network](https://doi.org/10.1109/RAAICON48939.2019.23), **Md Mahedi Hasan**, Md Shamimul Islam, Sohaib Abdullah <strong><span class='show_paper_citations' data='m3MlVBUAAAAJ:maZDTaKrznsC'></span></strong> 
- `ICBSLP 2019` [Aibangla: A Benchmark Dataset for Isolated Bangla Handwritten Basic and Compound Character Recognition](https://doi.org/10.1109/ICBSLP47725.2019.201481), **Md Mahedi Hasan**, Mahathir Mohammad Abir, et al.
- `ICBSLP 2018` [YOLO-Based Three-Stage Network for Bangla License Plate Recognition in Dhaka Metropolitan City](https://doi.org/10.1109/ICBSLP.2018.8554668), Sohaib Abdullah, **Md Mahedi Hasan**, Sheikh Muhammad Saiful Islam <strong><span class='show_paper_citations' data='m3MlVBUAAAAJ:HDshCWvjkbEC'></span></strong>
- `ICCIT 2018` [DEEPGONET: Multi-label Prediction of GO Annotation for Protein from Sequence Using Cascaded Convolutional and Recurrent Network](https://doi.org/10.1109/ICCITECHN.2018.8631921), Sheikh Muhammad Saiful Islam, **Md Mahedi Hasan** 
- `IJCSIS 2020` [Forensic Detection of Digital Image Tampering Using Statistical Analysis](https://doi.org/10.5281/zenodo.4130382), Md. Zahurul Haque, **Md Mahedi Hasan** 


# üìù Research Grants
## Multi-Finger Contactless Fingerprint Matching
-   PI Name: Jeremy M. Dawson, Nasser M. Nasrabadi
-   Name of Funding Organization: [CITer](https://citer.clarkson.edu/) (Project \#21S-04W),  IUCRC - NSF
-   Period of Grant Award: 1 Year (08/13/2021 - 08/12/2022)
-   Amount: $50,000
-   Project Title: **Evaluation of the Performance of Multi-Finger Contactless Fingerprint Matching**
-   <p align="justify">My Role in the Project: I developed an algorithm for multi-finger contactless fingerprint matching. I successfully achieved all project milestones under the supervision of the PIs. I presented the final report and webinar, along with publishing two academic papers based on the experimental results. </p>

## One-to-One Face Recognition
-   PI Name: Nasser M. Nasrabadi, Jeremy M. Dawson
-   Name of Funding Organization: [CITer](https://citer.clarkson.edu/) (Project #22S-06W),  IUCRC - NSF
-   Period of Grant Award: 1 Year (11/05/2022 - 04/21/2023)
-   Amount: $50,000
-   Project Title: **One-to-One Face Recognition with Human Examiner in the Loop**
-   <p align="justify">My Role in the Project: I developed a text-guided face recognition (FR) system to improve the performance of state-of-the-art FR algorithms by integrating facial attributes through natural language descriptions. I successfully met all project milestones under the supervision of the PIs. I presented both the progress report and the final report, and additionally published two academic papers.</p>

## Deep Face Recognition
-   PI Name: Nasser M. Nasrabadi, **Md Mahedi Hasan**
-   Name of Funding Organization: [CITer](https://citer.clarkson.edu/) (Project #22F-01W),  IUCRC - NSF
-   Period of Grant Award: 1 Year (09/03/2022 - 10/25/2023)
-   Amount: $50,000
-   Project Title: **A Perpetual Deep Face Recognition System**
-   <p align="justify">My Role in the Project: I wrote the proposal with Prof. Nasser. I designed the class-incremental learning framework which can learn and improve from a sequence of face recognition tasks without storing any exemplar sets. I successfully completed all the project milestones. I presented the progress and the final report. </p>


# üìù Research Projects
### Medical Image Classification
- Project Title: Early Detection and Grading of Diabetic Retinopathy Using Retinal Fundus Images
- Period: 1 Year (10/14/2017 - 10/30/2018)
- Description: We developed a novel deep convolutional neural network, which performs the early-stage detection by identifying all microaneurysms (MAs), the first signs of DR, along with correctly assigning labels to retinal fundus images which are graded into five categories. We have tested our network on the largest publicly available Kaggle diabetic retinopathy dataset, and achieved 0.851 quadratic weighted kappa score and 0.844 AUC score.
-  Resources: [\[paper\]](https://arxiv.org/abs/1812.10595) 

### License Plate Recognition
-  Project Title: Real-time Automatic Bangla License Plate Detection and Recognition
-  Period: 1 Year (05/01/2018 - 04/30/2019)
-  Description: We have developed a real-time automatic Bangla license plate recognition system based on YOLO-v3. Additionally, we curated a dataset comprising 1,500 diverse images of Bangladeshi vehicular license plates. These images were manually captured from streets, simulating various real-world scenarios. This project is funded by department of CSE, Manarat International University ([MIU](https://manarat.ac.bd/)).
-  Resources: [\[dataset\]](https://github.com/Mahedi-61/Bangla_License_Plate_Dataset), [\[paper\]](https://doi.org/10.1109/ICBSLP.2018.8554668) 

### Bangla Handwritten Character Recognition
-  Project Title: Deep Isolated Bangla Handwritten Basic and Compound Character Recognition
-  Period: 1 Year (01/12/2018 - 12/30/2018)
-  Description: We present AIBangla, a new benchmark image database for isolated handwritten Bangla characters with detailed usage and a performance baseline. Our dataset contains 80,403 hand-written images on 50 Bangla basic characters and 249,911 hand-written images on 171 Bangla compound characters which were written by more than 2,000 unique writers from various institutes across Bangladesh. This project is funded by department of CSE, Manarat International University ([MIU](https://manarat.ac.bd/))
-  Resources: [\[dataset\]](https://doi.org/10.17632/hf2tt9kxkn.1), [\[paper\]](https://doi.org/10.1109/ICBSLP47725.2019.201481) 

### Bangla Sign Language Recognition
-  Project Title: Deep Bangla Sign Language Recognition from Video: A New Large-scale Dataset
-  Period: 2 Year (11/05/2021 - 04/05/2023)
- Description: We have developed an attention-based Bi-GRU model that captures the temporal dynamics of pose information used by individuals communicating through sign language. Furthermore, we created a large-scale dataset called the MVBSL-W50, which comprises 50 isolated words across 13 categories. 
- Resources: [\[dataset\]](https://github.com/Mahedi-61/MV-BSDL), [\[paper\]](https://arxiv.org/abs/2302.11559)